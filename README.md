# <img src="[https://github.com/tensaeaschalew/deep-learning-assignment/blob/main/assets/animation.gif"](https://github.com/tensaeaschalew/deep-learning-assignment/blob/main/assets/animation.gif") width="50px"> Tensae Aschalew's Deep Learning Assignment Report

## Overview

This report presents the findings from a Deep Learning Assignment that explores fundamental aspects of neural network design using the MNIST dataset.

## Key Insights

* Activation Functions: ReLU resolves the vanishing gradient problem by maintaining a constant gradient for positive inputs, outperforming Sigmoid and Tanh in speed and accuracy.
* Network Depth: Increasing depth boosts representational power, with medium depths (4-8 layers) optimal for MNIST, balancing performance and efficiency.

## Experiments

The experiments involve training neural networks, analyzing their performance, and visualizing results to draw meaningful conclusions.

## Report Structure

The report is divided into three questions:

1. Activation Functions
2. Depth of Artificial Neural Networks
3. Summary of Key Insights

## Author

Tensae Aschalew
