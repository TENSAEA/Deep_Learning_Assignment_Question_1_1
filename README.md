<div align="center">
  <img src="https://github.com/tensaeaschalew/deep-learning-assignment/blob/main/assets/animation.gif" width="60px" />
  
  <br/>
  
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&weight=500&size=28&pause=1000&color=F75C7E&center=true&vCenter=true&width=700&lines=Deep+Learning+Report;MNIST+Analysis;Neural+Networks" alt="Typing SVG" />
</div>


---

### ğŸ“˜ Overview

> This report presents the findings from a **Deep Learning Assignment** that explores core aspects of neural network design using the **MNIST dataset**.

- Implementation using Python & PyTorch
- Focused on **activation functions** and **network depth**
- Includes **visualizations**, **performance analysis**, and **recommendations**

---

### ğŸ” Key Insights

| Topic | Summary |
|------|---------|
| ğŸš€ **Activation Functions** | **ReLU** solves the vanishing gradient problem and performs better than Sigmoid/Tanh in both speed and accuracy. |
| ğŸ§  **Network Depth** | Deeper networks offer more representation power. For MNIST, **4â€“8 layers** gives the best trade-off between accuracy and computational cost. |

---

### ğŸ§ª Experiments

- Designed multiple neural networks with varying depths
- Compared **ReLU**, **Sigmoid**, and **Tanh** functions
- Tracked performance metrics: accuracy, loss, convergence rate
- Visualized training history and final predictions

---

### ğŸ—‚ Report Structure

```plaintext
â”œâ”€â”€ Activation Functions
â”‚   â””â”€â”€ ReLU vs Sigmoid vs Tanh
â”œâ”€â”€ Depth of Neural Networks
â”‚   â””â”€â”€ Shallow vs Deep models
â””â”€â”€ Summary of Key Insights
